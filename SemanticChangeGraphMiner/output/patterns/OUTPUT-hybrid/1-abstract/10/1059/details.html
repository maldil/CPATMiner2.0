<html><h3>299280acd16069ad8da403d06abdc9b9eca0ef3e,yarll/agents/ppo/ppo.py,PPO,train,#PPO#,151
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            new_log_prob = self.new_network.log_prob(actions_taken, new_logits)
            old_log_prob = self.old_network.log_prob(actions_taken, old_logits)
            mean_actor_loss = -tf.reduce_mean(self._actor_loss(old_log_prob, new_log_prob, advantages))
            <a id="change">mean_critic_loss</a> = <a id="change">tf.reduce_mean(self._critic_loss(returns, values))</a>
            loss = mean_actor_loss + self.config["vf_coef"] * mean_critic_loss - \
                self.config["entropy_coef"] * tf.reduce_mean(self.new_network.entropy(new_logits))
            <a id="change">gradients</a> = tape.gradient(loss, self.new_network.trainable_weights)
        self.optimizer.apply_gradients(zip(gradients, self.new_network.trainable_weights))
        return mean_actor_loss, mean_critic_loss, loss, tf.linalg.global_norm(gradients), old_log_prob, new_log_prob, new_logits
</code></pre><h3>After Change</h3><pre><code class='java'>
            new_log_prob = self.new_network.log_prob(actions_taken, new_logits)
            old_log_prob = self.old_network.log_prob(actions_taken, old_logits)
            mean_actor_loss = -tf.reduce_mean(self._actor_loss(old_log_prob, new_log_prob, advantages))
            <a id="change">mean_critic_loss</a> = .5 * <a id="change">tf.reduce_mean(self._critic_loss(returns, values))</a>
            loss = mean_actor_loss + self.config["vf_coef"] * mean_critic_loss - \
                self.config["entropy_coef"] * tf.reduce_mean(self.new_network.entropy(new_logits))
            <a id="change">gradients</a> = tape.gradient(loss, self.new_network.trainable_weights)
        self.optimizer.apply_gradients(zip(gradients, self.new_network.trainable_weights))
        return mean_actor_loss, mean_critic_loss, loss, tf.linalg.global_norm(gradients), old_log_prob, new_log_prob, new_logits
</code></pre><img src="3880166.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 1</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/arnomoonens/yarll/commit/299280acd16069ad8da403d06abdc9b9eca0ef3e#diff-b6ad861197e3ed50385bc08dbb9454faf358b1c1140c6a806c07997faf1551e6L165' target='_blank'>Link</a></div><div id='project'> Project Name: arnomoonens/yarll</div><div id='commit'> Commit Name: 299280acd16069ad8da403d06abdc9b9eca0ef3e</div><div id='time'> Time: 2020-01-14</div><div id='author'> Author: arno.moonens@gmail.com</div><div id='file'> File Name: yarll/agents/ppo/ppo.py</div><div id='class'> Class Name: PPO</div><div id='method'> Method Name: train</div><BR>