<html><h3>78f46822aec6cea5e216e1f7404d351835c54346,yarll/agents/sac.py,SAC,train,#SAC#,92
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        next_value_batch = self.target_value(state1_batch)
        softq_targets = reward_batch + (1 - terminal1_batch) * \
            self.config["gamma"] * tf.reshape(next_value_batch, [-1])
        <a id="change">softq_targets</a> = tf.cast(<a id="change">tf.reshape(softq_targets, [self.config["batch_size"], 1])</a>, tf.float32)

        actions, action_logprob = self.actor_network(state0_batch)
        new_softq = self.softq_network(state0_batch, actions)

        with tf.GradientTape() as softq_tape:
            softq = self.softq_network(state0_batch, action_batch)
            <a id="change">softq_loss</a> = tf.reduce_mean(tf.square(softq - softq_targets))
        value_target = tf.stop_gradient(new_softq - action_logprob)
        with tf.GradientTape() as value_tape:
            values = self.value_network(state0_batch)
            value_loss = tf.reduce_mean(tf.square(values - value_target))
        advantage = tf.stop_gradient(action_logprob - new_softq + values)
        with tf.GradientTape() as actor_tape:
            _, action_logprob = self.actor_network(state0_batch)
            actor_loss = tf.reduce_mean(action_logprob * advantage)

        actor_gradients = actor_tape.gradient(actor_loss, self.actor_network.trainable_weights)
        <a id="change">softq_gradients</a> = softq_tape.gradient(softq_loss, self.softq_network.trainable_weights)
        value_gradients = value_tape.gradient(value_loss, self.value_network.trainable_weights)

        self.actor_optimizer.apply_gradients(zip(actor_gradients, self.actor_network.trainable_weights))</code></pre><h3>After Change</h3><pre><code class='java'>
        next_value_batch = self.target_value(state1_batch)
        softq_targets = reward_batch + (1 - terminal1_batch) * \
            self.config["gamma"] * tf.reshape(next_value_batch, [-1])
        <a id="change">softq_targets</a> = <a id="change">tf.reshape(softq_targets, [self.config["batch_size"], 1])</a>

        with tf.GradientTape() as softq_tape:
            softq = self.softq_network(state0_batch, action_batch)
            <a id="change">softq_loss</a> = tf.reduce_mean(tf.square(softq - softq_targets))
        with tf.GradientTape() as value_tape:
            values = self.value_network(state0_batch)
        with tf.GradientTape() as actor_tape:
            actions, action_logprob = self.actor_network(state0_batch)
            new_softq = self.softq_network(state0_batch, actions)
            advantage = tf.stop_gradient(action_logprob - new_softq + values)
            actor_loss = tf.reduce_mean(action_logprob * advantage)
        value_target = tf.stop_gradient(new_softq - action_logprob)
        with value_tape:
            value_loss = tf.reduce_mean(tf.square(values - value_target))

        actor_gradients = actor_tape.gradient(actor_loss, self.actor_network.trainable_weights)
        <a id="change">softq_gradients</a> = softq_tape.gradient(softq_loss, self.softq_network.trainable_weights)
        value_gradients = value_tape.gradient(value_loss, self.value_network.trainable_weights)

        self.actor_optimizer.apply_gradients(zip(actor_gradients, self.actor_network.trainable_weights))</code></pre><img src="3880260.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 1</div><BR><div id='size'>Non-data size: 2</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/arnomoonens/yarll/commit/78f46822aec6cea5e216e1f7404d351835c54346#diff-6b52b386c0b630c9582cbe2d41fb4eee1eb7f2ab2cec377dd486b5406f4ccb8dL90' target='_blank'>Link</a></div><div id='project'> Project Name: arnomoonens/yarll</div><div id='commit'> Commit Name: 78f46822aec6cea5e216e1f7404d351835c54346</div><div id='time'> Time: 2019-06-10</div><div id='author'> Author: arno.moonens@gmail.com</div><div id='file'> File Name: yarll/agents/sac.py</div><div id='class'> Class Name: SAC</div><div id='method'> Method Name: train</div><BR>